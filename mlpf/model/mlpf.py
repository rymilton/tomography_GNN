import math
import numpy as np
from typing import Union, List

import torch
import torch.nn as nn
from torch.nn.attention import SDPBackend, sdpa_kernel

from mlpf.logger import _logger
from mlpf.model.gnn_lsh import CombinedGraphLayer


def trunc_normal_(tensor, mean=0.0, std=1.0, a=-2.0, b=2.0):
    # From https://github.com/rwightman/pytorch-image-models/blob/
    #        18ec173f95aa220af753358bf860b16b6691edb2/timm/layers/weight_init.py#L8
    r"""Fills the input Tensor with values drawn from a truncated
    normal distribution. The values are effectively drawn from the
    normal distribution :math:`\mathcal{N}(\text{mean}, \text{std}^2)`
    with values outside :math:`[a, b]` redrawn until they are within
    the bounds. The method used for generating the random values works
    best when :math:`a \leq \text{mean} \leq b`.
    Args:
        tensor: an n-dimensional `torch.Tensor`
        mean: the mean of the normal distribution
        std: the standard deviation of the normal distribution
        a: the minimum cutoff value
        b: the maximum cutoff value
    Examples:
        >>> w = torch.empty(3, 5)
        >>> nn.init.trunc_normal_(w)
    """

    def norm_cdf(x):
        # Computes standard normal cumulative distribution function
        return (1.0 + math.erf(x / math.sqrt(2.0))) / 2.0

    with torch.no_grad():
        # Values are generated by using a truncated uniform distribution and
        # then using the inverse CDF for the normal distribution.
        # Get upper and lower cdf values
        lo = norm_cdf((a - mean) / std)
        up = norm_cdf((b - mean) / std)

        # Uniformly fill tensor with values from [l, u], then translate to
        # [2lo-1, 2up-1].
        tensor.uniform_(2 * lo - 1, 2 * up - 1)

        # Use inverse cdf transform for normal distribution to get truncated
        # standard normal
        tensor.erfinv_()

        # Transform to proper mean, std
        tensor.mul_(std * math.sqrt(2.0))
        tensor.add_(mean)

        # Clamp to ensure it's in the proper range
        tensor.clamp_(min=a, max=b)
        return tensor


def get_activation(activation):
    if activation == "elu":
        act = nn.ELU
    elif activation == "relu":
        act = nn.ReLU
    elif activation == "relu6":
        act = nn.ReLU6
    elif activation == "leakyrelu":
        act = nn.LeakyReLU
    elif activation == "gelu":
        act = nn.GELU
    return act


class SimpleMultiheadAttention(nn.MultiheadAttention):
    def __init__(
        self,
        embed_dim: int,
        num_heads: int,
        dropout: float = 0.0,
        device=None,
        dtype=None,
        export_onnx_fused=False,
    ) -> None:
        factory_kwargs = {"device": device, "dtype": dtype}
        bias = True
        batch_first = True
        super().__init__(embed_dim, num_heads, dropout, bias=bias, batch_first=batch_first, **factory_kwargs)
        self.head_dim = int(embed_dim // num_heads)
        self.out_proj = nn.Linear(self.embed_dim, self.embed_dim, bias=bias, **factory_kwargs)
        self.export_onnx_fused = export_onnx_fused

    def forward(self, q: torch.Tensor, k: torch.Tensor, v: torch.Tensor, need_weights=False, key_padding_mask=None) -> torch.Tensor:
        # q, k, v: 3D tensors (batch_size, seq_len, embed_dim), embed_dim = num_heads*head_dim
        bs, seq_len, embed_dim = q.size()
        head_dim = self.head_dim
        num_heads = self.num_heads

        # split stacked in_proj_weight, in_proj_bias to q, k, v matrices
        wq, wk, wv = torch.split(self.in_proj_weight.data, [self.embed_dim, self.embed_dim, self.embed_dim], dim=0)
        bq, bk, bv = torch.split(self.in_proj_bias.data, [self.embed_dim, self.embed_dim, self.embed_dim], dim=0)

        q = torch.matmul(q, wq.T) + bq
        k = torch.matmul(k, wk.T) + bk
        v = torch.matmul(v, wv.T) + bv

        # for pytorch internal scaled dot product attention, we need (bs*num_heads, seq_len, head_dim)
        if not self.export_onnx_fused:
            q = q.reshape(bs, seq_len, num_heads, head_dim).transpose(1, 2).reshape(bs * num_heads, seq_len, head_dim)
            k = k.reshape(bs, seq_len, num_heads, head_dim).transpose(1, 2).reshape(bs * num_heads, seq_len, head_dim)
            v = v.reshape(bs, seq_len, num_heads, head_dim).transpose(1, 2).reshape(bs * num_heads, seq_len, head_dim)

        # this function will have different shape signatures in native pytorch sdpa and in ONNX com.microsoft.MultiHeadAttention
        # in pytorch: (bs*num_heads, seq_len, head_dim)
        # in ONNX: (bs, seq_len, num_heads*head_dim)
        attn_output = torch.nn.functional.scaled_dot_product_attention(q, k, v, dropout_p=self.dropout)

        # in case running with pytorch internal scaled dot product attention, reshape back to the original shape
        if not self.export_onnx_fused:
            attn_output = attn_output.reshape(bs, num_heads, seq_len, head_dim).transpose(1, 2).reshape(bs, seq_len, num_heads * head_dim)

        # assert list(attn_output.size()) == [bs, seq_len, num_heads * head_dim]
        attn_output = self.out_proj(attn_output)
        return attn_output, None


class PreLnSelfAttentionLayer(nn.Module):
    def __init__(
        self,
        name="",
        activation="elu",
        embedding_dim=128,
        num_heads=2,
        width=128,
        dropout_mha=0.1,
        dropout_ff=0.1,
        attention_type="efficient",
        learnable_queries=False,
        elems_as_queries=False,
        use_simplified_attention=False,
        export_onnx_fused=False,
        save_attention=False,
    ):
        super(PreLnSelfAttentionLayer, self).__init__()
        self.name = name

        self.use_simplified_attention = use_simplified_attention

        self.attention_type = attention_type
        self.act = get_activation(activation)

        if self.use_simplified_attention:
            self.mha = SimpleMultiheadAttention(embedding_dim, num_heads, dropout=dropout_mha, export_onnx_fused=export_onnx_fused)
        else:
            self.mha = torch.nn.MultiheadAttention(embedding_dim, num_heads, dropout=dropout_mha, batch_first=True)

        self.norm0 = torch.nn.LayerNorm(embedding_dim)
        self.norm1 = torch.nn.LayerNorm(embedding_dim)
        self.seq = torch.nn.Sequential(nn.Linear(embedding_dim, width), self.act(), nn.Linear(width, embedding_dim), self.act())
        self.dropout = torch.nn.Dropout(dropout_ff)

        if not self.use_simplified_attention:
            _logger.info("layer {} using attention_type={}".format(self.name, attention_type))
            self.attn_params = {
                "math": [SDPBackend.MATH],
                "efficient": [SDPBackend.EFFICIENT_ATTENTION],
                "flash": [SDPBackend.FLASH_ATTENTION],
            }

        self.learnable_queries = learnable_queries
        self.elems_as_queries = elems_as_queries
        if self.learnable_queries:
            self.queries = nn.Parameter(torch.zeros(1, 1, embedding_dim), requires_grad=True)
            trunc_normal_(self.queries, std=0.02)

        # options for saving the attention matrix
        self.save_attention = save_attention
        self.att_mat_idx = 0
        self.outdir = "."

    def forward(self, x, mask, initial_embedding):
        if mask is not None:
            mask_ = mask.unsqueeze(-1)

        residual = x
        x_norm = self.norm0(x)

        q = x_norm

        if self.learnable_queries:
            q = self.queries.expand(*x.shape)
        elif self.elems_as_queries:
            q = initial_embedding
        if mask is not None:
            q = q * mask_

        if self.use_simplified_attention:
            mha_out = self.mha(q, x_norm, x_norm, need_weights=False)[0]
        else:
            with sdpa_kernel(self.attn_params[self.attention_type]):
                mha_out = self.mha(q, x_norm, x_norm, need_weights=False)[0]

                if self.save_attention:
                    att_mat = self.mha(q, x_norm, x_norm, need_weights=True)[1]
                    att_mat = att_mat.detach().cpu().numpy()

        x = residual + mha_out
        residual = x
        x_norm = self.norm1(x)
        ffn_out = self.seq(x_norm)
        ffn_out = self.dropout(ffn_out)
        if mask is not None:
            x = x * mask_
        if not self.use_simplified_attention and self.save_attention:
            np.savez(
                open("{}/attn_{}_{}.npz".format(self.outdir, self.name, self.att_mat_idx), "wb"),
                att=att_mat,
                x=x.detach().cpu().numpy(),
                in_proj_weight=self.mha.in_proj_weight.detach().cpu().numpy(),
            )
        return x


def ffn(input_dim, output_dim, width, act, dropout):
    return nn.Sequential(
        nn.Linear(input_dim, width),
        act(),
        torch.nn.LayerNorm(width),
        nn.Dropout(dropout),
        nn.Linear(width, output_dim),
    )


# The head to predict voxel densities for each muon. Output will be padded to the max number of voxels.
class VoxelDensityHead(nn.Module):
    def __init__(
        self,
        embed_dim,
        num_voxels,
        width,
        act,
        dropout=0.0,
    ):
        super().__init__()

        self.num_voxels = num_voxels

        self.net = nn.Sequential(
            nn.Linear(embed_dim, width),
            act(),
            nn.Dropout(dropout),
            nn.Linear(width, width),
            act(),
            nn.Dropout(dropout),
            nn.Linear(width, num_voxels),
        )

    def forward(self, embedding, muon_mask=None):
        """
        embedding: (B, N_muons, embed_dim)
        muon_mask:      (B, N_muons) or None
        """
        out = self.net(embedding)  # (B, N_muons, num_voxels)

        # Removing padded muons 
        if muon_mask is not None:
            out = out * muon_mask.unsqueeze(-1)

        return out



class MLPF(nn.Module):
    def __init__(
        self,
        input_dim=34,
        num_classes=8,
        embedding_dim=128,
        width=128,
        num_convs=2,
        dropout_ff=0.0,
        activation="elu",
        layernorm=True,
        conv_type="attention",
        input_encoding="joint",
        num_padded_voxels = 128,
        # element types which actually exist in the dataset
        elemtypes_nonzero=[1, 4, 5, 6, 8, 9, 10, 11],
        # should the conv layer outputs be concatted (concat) or take the last (last)
        learned_representation_mode="last",
        # gnn-lsh specific parameters
        bin_size=640,
        max_num_bins=200,
        distance_dim=128,
        num_node_messages=2,
        ffn_dist_hidden_dim=128,
        ffn_dist_num_layers=2,
        # self-attention specific parameters
        num_heads=16,
        head_dim=16,
        attention_type="flash",
        dropout_conv_reg_mha=0.0,
        dropout_conv_reg_ff=0.0,
        dropout_conv_id_mha=0.0,
        dropout_conv_id_ff=0.0,
        use_pre_layernorm=False,
        use_simplified_attention=False,
        export_onnx_fused=False,
        save_attention=False,
    ):
        super(MLPF, self).__init__()

        self.conv_type = conv_type

        self.act = get_activation(activation)

        self.learned_representation_mode = learned_representation_mode

        self.input_encoding = input_encoding

        self.input_dim = input_dim
        self.num_convs = num_convs

        self.bin_size = bin_size
        self.elemtypes_nonzero = elemtypes_nonzero

        self.use_pre_layernorm = use_pre_layernorm

        if self.conv_type == "attention":
            embedding_dim = num_heads * head_dim
            width = num_heads * head_dim

        self.nn0 = ffn(self.input_dim, embedding_dim, width, self.act, dropout_ff)

        if self.num_convs != 0:
            if self.conv_type == "attention":
                self.conv_id = nn.ModuleList()
                self.conv_reg = nn.ModuleList()

                for i in range(self.num_convs):
                    lastlayer = i == self.num_convs - 1
                    self.conv_id.append(
                        PreLnSelfAttentionLayer(
                            name="conv_id_{}".format(i),
                            activation=activation,
                            embedding_dim=embedding_dim,
                            num_heads=num_heads,
                            width=width,
                            dropout_mha=dropout_conv_id_mha,
                            dropout_ff=dropout_conv_id_ff,
                            attention_type=attention_type,
                            elems_as_queries=lastlayer,
                            # learnable_queries=lastlayer,
                            use_simplified_attention=use_simplified_attention,
                            export_onnx_fused=export_onnx_fused,
                            save_attention=save_attention,
                        )
                    )
                    self.conv_reg.append(
                        PreLnSelfAttentionLayer(
                            name="conv_reg_{}".format(i),
                            activation=activation,
                            embedding_dim=embedding_dim,
                            num_heads=num_heads,
                            width=width,
                            dropout_mha=dropout_conv_reg_mha,
                            dropout_ff=dropout_conv_reg_ff,
                            attention_type=attention_type,
                            elems_as_queries=lastlayer,
                            # learnable_queries=lastlayer,
                            use_simplified_attention=use_simplified_attention,
                            export_onnx_fused=export_onnx_fused,
                            save_attention=save_attention,
                        )
                    )
            elif self.conv_type == "gnn_lsh":
                self.conv = nn.ModuleList()
                for i in range(self.num_convs):
                    gnn_conf = {
                        "inout_dim": embedding_dim,
                        "bin_size": self.bin_size,
                        "max_num_bins": max_num_bins,
                        "distance_dim": distance_dim,
                        "layernorm": layernorm,
                        "num_node_messages": num_node_messages,
                        "dropout": dropout_ff,
                        "ffn_dist_hidden_dim": ffn_dist_hidden_dim,
                        "ffn_dist_num_layers": ffn_dist_num_layers,
                    }
                    self.conv.append(CombinedGraphLayer(**gnn_conf))

        if self.learned_representation_mode == "concat":
            decoding_dim = self.num_convs * embedding_dim
        elif self.learned_representation_mode == "last":
            decoding_dim = embedding_dim

        # elementwise DNN for node momentum regression
        embed_dim = decoding_dim
        self.nn_voxel_density = VoxelDensityHead(
            embed_dim=decoding_dim,
            num_voxels=num_padded_voxels,
            width=width,
            act=self.act,
            dropout=dropout_ff,
        )

        if self.use_pre_layernorm:  # add final norm after last attention block as per https://arxiv.org/abs/2002.04745
            self.final_norm_id = torch.nn.LayerNorm(decoding_dim)
            self.final_norm_reg = torch.nn.LayerNorm(embed_dim)

    # @torch.compile
    def forward(self, X_features, mask):
        """
        Args:
            X_features : (B, M, input_dim) input features per muon
            mask       : (B, M)          valid muons
        Returns:
            preds_voxel_density : (B, M, N) predicted densities along each muon
        """

        Xfeat_normed = X_features
        embeddings = []

        # ----- Input embedding -----
        if self.input_encoding == "joint":
            embedding = self.nn0(Xfeat_normed)  # single embedding for all muons

        elif self.input_encoding == "split":
            # Element-type-specific embeddings
            # Shape: (B, M, embed_dim, n_elemtypes)
            embedding = torch.stack([nn0(Xfeat_normed) for nn0 in self.nn0], dim=-1)

            # Shape: (B, M, n_elemtypes)
            elemtype_mask = torch.cat(
                [X_features[..., 0:1] == elemtype for elemtype in self.elemtypes_nonzero],
                dim=-1,
            )

            # Select correct element-type embedding
            embedding = torch.sum(embedding * elemtype_mask.unsqueeze(-2), dim=-1)

        # ----- Convolution / attention layers -----
        if self.num_convs > 0:
            for i, conv in enumerate(self.conv):
                conv_input = embedding if i == 0 else embeddings[-1]
                out = conv(conv_input, mask, embedding)
                embeddings.append(out)
        else:
            embeddings.append(embedding)

        # ----- Aggregate learned representation -----
        if self.learned_representation_mode == "concat":
            final_embedding = torch.cat(embeddings, dim=-1)
        elif self.learned_representation_mode == "last":
            final_embedding = embeddings[-1]

        if self.use_pre_layernorm:
            final_embedding = self.final_norm_reg(final_embedding)

        # ----- Voxel-density prediction -----
        preds_voxel_density = self.nn_voxel_density(final_embedding)

        return preds_voxel_density


def configure_model_trainable(model: MLPF, trainable: Union[str, List[str]], is_training: bool):
    """Set only the given layers as trainable in the model"""

    if isinstance(model, torch.nn.parallel.DistributedDataParallel):
        raise Exception("configure trainability before distributing the model")
    if is_training:
        model.train()
        if trainable != "all":
            model.eval()

            # first set all parameters as non-trainable
            for param in model.parameters():
                param.requires_grad = False

            # now explicitly enable specific layers
            for layer in trainable:
                layer = getattr(model, layer)
                layer.train()
                for param in layer.parameters():
                    param.requires_grad = True
    else:
        model.eval()
